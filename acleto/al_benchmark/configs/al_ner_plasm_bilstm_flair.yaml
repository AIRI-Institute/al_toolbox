output_dir: './workdir/run_active_learning'
hydra:
  run:
    dir: ${output_dir}/${now:%Y-%m-%d}/${now:%H-%M-%S}_${seed}_${to_string:${acquisition_model.checkpoint}}_${to_string:${successor_model.checkpoint}}
  sweep:
    dir: ${output_dir}/${now:%Y-%m-%d}/${now:%H-%M-%S}_${seed}_${to_string:${acquisition_model.checkpoint}}_${to_string:${successor_model.checkpoint}}
    subdir: ${hydra.job.num}
    
seed: 42
cuda_device: 0
cache_dir: ./workdir/cache_${seed}_${to_string:${acquisition_model.checkpoint}}
cache_model_and_dataset: False
time_dir: ./workdir/cache_${seed}_${to_string:${acquisition_model.checkpoint}}
dump_model: True
framework: ???
task: 'ner'
offline_mode: False

data:
    dataset_name: 'conll2003'
    path: 'datasets'
    tag_index: 2
    text_name: 'tokens'
    label_name: 'ner_tags'
    labels_to_remove:
    ner_encoding: 'iobes'

acquisition_model:
    type: ${task}
    checkpoint: 'distilbert-base-cased'
    tokenizer_max_length:
    classifier_dropout: 0.
    exists_in_repo: True # whether the model exists in HF models repo
    path_to_pretrained: # required if the model does not exist in HF models repo
    training:
        transformers_output_dir: ./acquisition_seed_${seed} # needed when using transformers wrapper
        dev_size: 0.
        shuffle_dev: False
        freeze_embedder: False
        batch_size_args:
            batch_size: 16
            eval_batch_size: 100
            min_num_gradient_steps: 350
            adjust_batch_size: True
            adjust_num_epochs: True
            min_batch_size: 4
        trainer_args:
            num_epochs: 5
            patience: ${get_patience_value:${acquisition_model.training.dev_size}}
            grad_clipping: 1.
            serialization_dir: ./output/${to_string:${acquisition_model.checkpoint}}_${seed}_${now:%Y-%m-%d}_${now:%H-%M-%S}/acquisition
            validation_metric: ${framework.validation_metric_ner}
            evaluation_strategy: 'epoch' # for transformers wrapper, 'epoch' or 'steps' or 'no'
            eval_metrics:
            fp16:
                training: False
                evaluation: False
            accumulation:
                gradient_accumulation_steps: 1
                eval_accumulation_steps:
        optimizer_args:
            weight_decay: 0.01
            lr: 5e-5
        scheduler_args:
            warmup_steps_factor: 0.1
            use_adafactor: False

successor_model:
    type: ${task}
    checkpoint: 'bert-base-cased'
    tokenizer_max_length:
    classifier_dropout: 0.
    exists_in_repo: True # whether the model exists in HF models repo
    path_to_pretrained: # required if the model does not exist in HF models repo
    training:
        transformers_output_dir: ./successor_seed_${seed} # needed when using transformers wrapper
        dev_size: 0.1
        shuffle_dev: False
        freeze_embedder: False
        batch_size_args:
            batch_size: 16
            eval_batch_size: 100
            min_num_gradient_steps: 350
            adjust_batch_size: True
            adjust_num_epochs: True
            min_batch_size: 4
        trainer_args:
            num_epochs: 5
            patience: ${get_patience_value:${successor_model.training.dev_size}}
            grad_clipping: 1.
            serialization_dir: ./output/${to_string:${acquisition_model.checkpoint}}_${seed}_${now:%Y-%m-%d}_${now:%H-%M-%S}/successor
            validation_metric: ${framework.validation_metric_ner}
            evaluation_strategy: 'epoch' # for transformers wrapper, 'epoch' or 'steps' or 'no'
            eval_metrics:
            fp16:
                training: False
                evaluation: False
            accumulation:
                gradient_accumulation_steps: 1
                eval_accumulation_steps:
        optimizer_args:
            weight_decay: 0.01
            lr: 5e-5
        scheduler_args:
            warmup_steps_factor: 0.1
            use_adafactor: False

target_model:
    type: ${task}
    checkpoint: 'bilstm-crf'
    embedding_dim: 100
    embedding_file: 'glove'
    embedding_trainable: True
    lstm:
        hidden_size: 200
        num_layers: 1
        recurrent_dropout_probability: 0.3 # locked dropout
        layer_dropout_probability: 0.2 # word dropout
        dropout: 0.0 # common dropout
        reproject_embeddings: False # bool or number
    char_emb:
        emb_dim: 30
        cnn_dim: 30
        cnn_filters: [2,3] #, 4, 5]
        activation: 'mish'
        is_sparse: False
        dropout: 0.0 # input dropout
    dropout: 0.0
    feedforward:
        activation: 'tanh'
        dropout: 0.3
        hidden_size: 200
        num_layers: 1
    tokenizer_max_length:
    training:
        transformers_output_dir: ./target_seed_${seed}
        dev_size: 0.
        shuffle_dev: False
        freeze_embedder: False
        batch_size_args:
            batch_size: 32
            eval_batch_size: 100
            min_num_gradient_steps: 350
            adjust_batch_size: True
            adjust_num_epochs: True
            min_batch_size: 32
        trainer_args:
            num_epochs: 50
            patience: 3
            grad_clipping: 5.
            serialization_dir: ./output/${to_string:${acquisition_model.checkpoint}}_${seed}_${now:%Y-%m-%d}_${now:%H-%M-%S}/target
            validation_metric: ${framework.validation_metric_ner}
            fp16:
                training: False
                evaluation: False
        optimizer_args:
            #weight_decay: 0.00001
            lr: 0.015
        scheduler_args:
            warmup_steps_factor: 0.3
            mode: 'max'
            factor: 0.05
            patience: 3
            verbose: True
            min_lr: 0.0001

al:
    strategy: 'mnlp'
    num_queries: 15
    init_p_or_n: 0.02
    step_p_or_n: 0.02
    split_by_tokens: True
    gamma_or_k_confident_to_save: 0.25
    T: 0.1
    sampling_type:
    iters_to_recalc_scores: [0, 1, 4, 8]
    evaluate_query: True
    strategy_kwargs:

tracin:
    use: True
    quantiles: [-1, 0.01, 0.025, 0.05, 0.1, 0.2]
    max_num_processes: 5
    num_model_checkpoints: 3
    nu: 1

defaults:
  - framework: flair # 'allennlp' or 'transformers'
